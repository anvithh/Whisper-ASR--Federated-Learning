{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLING PACKAGES"
      ],
      "metadata": {
        "id": "sJLXfrHTiw29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug7wRTTpikiC"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install jiwer\n",
        "!pip install datasets\n",
        "!pip install lora\n",
        "!pip install torch\n",
        "!pip install evaluate\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING PACKAGES"
      ],
      "metadata": {
        "id": "v_Nesp8KjcRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "from transformers import WhisperTokenizer\n",
        "from transformers import WhisperProcessor\n",
        "import lora\n",
        "import torch\n",
        "import evaluate\n",
        "import torch.nn as nn\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from peft import LoraConfig,get_peft_model,TaskType\n",
        "from datasets import load_dataset,DatasetDict\n",
        "from datasets import Audio\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import Seq2SeqTrainingArguments"
      ],
      "metadata": {
        "id": "osqbwtQzjej7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING PRETRAINED MODEL"
      ],
      "metadata": {
        "id": "seT4eJlqjjBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - Load Feature extractor: WhisperFeatureExtractor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"/content/tamilnew2\")\n",
        "# - Load Tokenizer: WhisperTokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Tamil\", task=\"transcribe\")\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Tamil\", task=\"transcribe\")\n"
      ],
      "metadata": {
        "id": "oWAlzFOAjmw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING DATASET"
      ],
      "metadata": {
        "id": "ZDzOnKE5juIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = load_dataset(\n",
        "    'csv', data_files={\n",
        "        'train': './dup.csv',\n",
        "        'test':'./duptest.csv'\n",
        "    }\n",
        ")\n",
        "print(common_voice)\n"
      ],
      "metadata": {
        "id": "JIHSFjhVjvyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETTING PEFT CONFIGURATIONS\n"
      ],
      "metadata": {
        "id": "gyzm2rpPjxl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,target_modules=[\"q_proj\",\"v_proj\"])\n",
        "\n",
        "config=LoraConfig(task_type=\"CAUSAL_LM\",\n",
        "                        r=16,\n",
        "                        lora_alpha=32,\n",
        "                        lora_dropout=0.05,\n",
        "                        bias=\"none\",\n",
        "                        target_modules=[\"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\",]\n",
        "                        )\n",
        "\n",
        "model=get_peft_model(model,peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "Qoid8ihtj0Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPARING DATASET\n"
      ],
      "metadata": {
        "id": "rdsEYSq1j5bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('| Check the random audio example from Common Voice dataset to see what form the data is in:')\n",
        "print(f'{common_voice[\"train\"][0]}\\n')\n",
        "\n",
        "# -> (1): Downsample from 48kHZ to 16kHZ\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "print('| Check the effect of downsampling:')\n",
        "print(f'{common_voice[\"train\"][0]}\\n')\n",
        "def prepare_dataset(batch):\n",
        "    \"\"\"\n",
        "    Prepare audio data to be suitable for Whisper AI model.\n",
        "    \"\"\"\n",
        "    # (1) load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # (2) compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # (3) encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch\n",
        "common_voice = common_voice.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=common_voice.column_names[\"train\"],\n",
        "    num_proc=2 # num_proc > 1 will enable multiprocessing\n",
        "    )"
      ],
      "metadata": {
        "id": "g1i0xnejkKOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETTING TRAINING ARGUMENTS"
      ],
      "metadata": {
        "id": "ngGx-vG2kQCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "\n",
        "    output_dir=\"./tamilnew2\",\n",
        "evaluation_strategy=\"steps\",\n",
        "eval_steps=100,\n",
        "per_device_train_batch_size=2,\n",
        "per_device_eval_batch_size=2,\n",
        "save_steps=1000,\n",
        "save_total_limit=2,\n",
        "num_train_epochs=10,\n",
        "logging_dir=\"./logs\",\n",
        "logging_steps=2,  # Adjusted to log more frequently\n",
        "report_to=[\"tensorboard\"],\n",
        "load_best_model_at_end=True,\n",
        "metric_for_best_model=\"wer\",\n",
        "greater_is_better=False,\n",
        "logging_first_step=True,\n",
        ")"
      ],
      "metadata": {
        "id": "bTsZqvewkWB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING FUNCTIONS"
      ],
      "metadata": {
        "id": "-YviM_J-km-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    \"\"\"\n",
        "    Use Data Collator to perform Speech Seq2Seq with padding\n",
        "    \"\"\"\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n"
      ],
      "metadata": {
        "id": "DIchVYGAkllp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION WITH 'WER'"
      ],
      "metadata": {
        "id": "3SKEo-Yyk2jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"wer\")\n",
        "# from transformers import WhisperForConditionalGeneration\n",
        "# model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ],
      "metadata": {
        "id": "iIjvrOqSk6Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMPUTE_METRICS FUNCTION"
      ],
      "metadata": {
        "id": "0uN49qJXlDfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Define evaluation metrics. We will use the Word Error Rate (WER) metric.\n",
        "    For more information, check:\n",
        "    \"\"\"\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "emRd300zlC8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING AND SAVING THE MODEL\n"
      ],
      "metadata": {
        "id": "PrESNvt2kjCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Training is started.')\n",
        "trainer.train()\n",
        "print('Training is finished.')\n",
        "\n",
        "trainer.save_model(\"./tamilnew2_finetuned\")\n",
        "\n",
        "tokenizer.save_pretrained(\"./tamilnew2_finetuned\")"
      ],
      "metadata": {
        "id": "R_WD-qmckk0N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}